In this section we'll explain our work on generation of binary trees
at random. We're interested to setup a simulation to study the means
of the number of leaves in trees with $n$ nodes and comparing the
result of the simulation against theoretical results.

In order to do this we've implemented an algorithm to generate binary
trees: it consume $n \in \mathbb{N} $ and produce a binary tree with
$n$ nodes.

We've repeated the application of that algorithm $k (>>
n)$ times in order to check if the algorithm is an \emph{uniform}
binary tree generator, that is, if the generator would be perfect,
each tree with $n$ nodes should have $ \frac{1}{
  \frac{1}{n+1}{{2n}\choose{n}} } $ probability to be generated.

The last point of our work is to check a theoretical result about the
mean of number of leaves among binary trees with $n$ nodes.


\section{Atkinson and Sack algorithm}
The set of binary trees with $n$ nodes is in one-to-one correspondence
with many other sets of combinatorial objects, one of them is the set
of well-formed bracket sequences with $n$ pairs of brackets. The
Atkinson and Sack algorithm focus on generating those sequences: in
this section we'll explain in our words their solution.

\subsection{Definitions}

A sequence is a word $w \in \{(,)\}^*$. In the following we draw a
sequence as a zigzag walk which starts from a base line (represented
as \texttt{-----}) and for each $( \in w$ we draw the character
slash, for each $) \in w$ we draw the character backslash. For
instance, the sequence of brackets $())()((())$ is drawn as:
\begin{verbatim}
          /\
 /\      /  \
--------------
    \/\/ 
\end{verbatim}
A word is \emph{balanced} if it contains an equal number of $($ and
$)$, as in the sequence above. Graphically speaking, balanced words
are sequences that start from the base line and, at right-most, return
to the base line (in the middle it is possible to cross zero, one or
more time the base line).\\

A word $w$ is \emph{reducible} if $w = w_1 w_2$ where $w_1$ and $w_2$
are \emph{balanced} and \emph{non empty}. For instance the sequence
reported above is reducible in four words while the following one
isn't:
\begin{verbatim}

   /\
  /  \/\ 
 /      \
----------
\end{verbatim}

A \emph{balanced} word $w$ has \emph{defect} $i$ if $w$ has $i$
bracket pairs under the base line. Words with \emph{defect} $0$ are
called \emph{well-formed}. The previous word has \emph{defect} $0$,
while the following one has \emph{defect} $3$:
\begin{verbatim}
 /\        /\
----------------
    \/\  /
       \/
\end{verbatim}

A word $w^*$ is the complement of a word $w$ if $\forall i \in
\{0,\ldots,|w|\}: w[i]=( \leftrightarrow w^*[i]=)$, for instance the
complement of the previous word is:
\begin{verbatim}
       /\
    /\/  \   
--------------
 \/        \/ 
\end{verbatim}

If a \emph{balanced} word $w$ is \emph{non reducible} then either $w$
or $w^*$ is \emph{well-formed}. We can see this result assuming $w$ be
one of the following words:
\begin{verbatim}
            ----------
   /\        \      /
  /  \/\      \  /\/
 /      \      \/
----------
\end{verbatim}
Both of them are \emph{balanced} and \emph{non reducible}: if $w$ is
the left-most one then $w$ is \emph{well-formed}, if $w$ is the
right-most one then $w^*$ is \emph{well-formed}.

\subsection{Splitting a \emph{reducible} word}
How can we decide if a word $w$ is \emph{reducible} or not? We can do
a cumulative summation on $w$ where we consider each $($ as $1$ and
each $)$ as $-1$. The following word has the cumulative sums
$(0,-1,0,1,0,1,2,1,0,-1,0)$:
\begin{verbatim}
       /\
    /\/  \   
--------------
 \/        \/ 
\end{verbatim}

The following words have cumulative sums $(0,1,2,3,2,1,2,1,0)$ and\\
$(0,-1,-2,-3,-2,-1,-2,-1,0)$ respectively:
\begin{verbatim}
            ----------
   /\        \      /
  /  \/\      \  /\/
 /      \      \/
----------
\end{verbatim}
Let $w$ be a word and $s = (0, s_1, \ldots, s_n)$ be the sequence of
cumulative sums respect of $w$, where $n = |w|$. We split $w= uv$ such
that $|u|>0 \wedge |v| \geq 0$ using the following strategy:
\begin{displaymath}
  \begin{split}
    i &=    \min\{k\in\{1,\ldots,n\}:s_k = 0 \}\\
    u &= (w_1,\ldots,w_i) \quad v = (w_{i+1},\ldots,w_n)
  \end{split}
\end{displaymath}

For instance, the following sequence as cumulative sums
$(0,1,0,1,2,1,0)$, hence $i = 2$ so $u=()$ and $v=(())$:
\begin{verbatim}
    /\
 /\/  \   
--------
\end{verbatim}
While the following one as cumulative sums $(0, 1, 2, 3, 2, 1,
2,1,0)$, hence $i = 8$ so $u=((())())$ and $v$ empty:
\begin{verbatim}
   /\
  /  \/\
 /      \ 
----------
\end{verbatim}

\subsection{Generating a \emph{balanced} word}

In order to generate a \emph{balanced} word of length $2n$ (not
necessary \emph{well-formed}) we use the follow strategy:
\begin{enumerate}
\item generate a uniform sample $L$ of length $n$ from
  $\{1,\ldots,2n\}$;
\item let $w = w_1w_2\ldots w_{2n}$ a word such that $w_i = (
  \leftrightarrow i \in L$;
\end{enumerate}

\subsection{Transform a \emph{balanced} word in a \emph{well-formed}
  word}

Let $w=uv$ a \emph{balanced} word obtained using the strategy described
in the previous section. To get a \emph{well-formed} word from $w$ we
define a function $\phi:\{(,)\}^*\rightarrow \{(,)\}^*$ inductively as
follow ($\epsilon$ represent the empty string):
\begin{displaymath}
  \begin{split}
    \phi(\epsilon) &= \epsilon \\
    \phi(w) &= u \phi(v) \quad \text{if } u \text{ is
      \emph{well-formed}}\\
    \phi(w) &= ( \phi(v) )t^* \quad \text{if } u=)t( \text{ is
      \emph{not well-formed}} 
  \end{split}
\end{displaymath}
In order to recognize if a word $w$ is \emph{well-formed} it is
sufficient to compute the comulative sequence of sums $s$ and check
that $\forall i:s_i \geq 0$. Let's do some examples:
\begin{verbatim}
      /\
----------
 \  /
  \/
\end{verbatim}
The previous sequence represents the word $w=))((()$. It is
\emph{balanced} so applying $\phi$ we get $\phi(w) = (\phi(v))()$
where $u=)t($, $t=)($ and $v=()$. Now $\phi(v) = ()$ because $v=()$ is
\emph{well-formed}, hence $\phi(w) = (())()$.

Another limit example:
\begin{verbatim}
----------
 \      /
  \  /\/
   \/
\end{verbatim}
The previous sequence represents the word $w=)))(()(($. It is
\emph{balanced} so applying $\phi$ we get $\phi(w) = (\phi(v))(())()$
where $u=)t($, $t=))(()($ and $v=\epsilon$. Now $\phi(v) =
\epsilon$ hence $\phi(w) = ()(())()$, graphically:
\begin{verbatim}
    /\
 /\/  \/\
------------
\end{verbatim}


\section{Implementation using R}

\begin{lstlisting}
  generate.tree <- function(number_of_nodes){
    word_dimension <- 2 * number_of_nodes    
    universe <- 1:word_dimension
    sample <- sample(universe, size=number_of_nodes)
    w = rep(0, word_dimension)
    for (i in 1:word_dimension) {
      w[i] <- ifelse(any(sample == i), 1, -1)
    }    
    phi=phi(w)
    list(word=w, phi=phi, as_brackets = brackets_of_word(phi))
  }

  split.word <- function(w){
    if(length(w) == 0){
      return(list(u=c(), v=c()))
    }    
    u_index_set <- 1:match(0, cumsum(w))
    list(u=w[u_index_set], v=w[-u_index_set])
  }

  phi <- function(w){
    if(length(w) == 0){
      return(w)
    }    
    split <- split.word(w)     
    if(all(cumsum(split$u) > -1)){
      return (c(split$u, phi(split$v)))
    }
    else{
      t = split$u[-c(1, length(split$u))]
      return (c(1, phi(split$v), -1, -t))
    }
  }
\end{lstlisting}

\section{Checking randomness}

In order to establish if the algorithm is an \emph{uniform random}
generator, we perform a $\chi^2$ test on the generated trees. We've
implemented that process in R and we're going to comment the result
obtained.

In \autoref{table:p-values-per-nodes} we report the $p$-values
obtained performing a simulation on trees with $4,5,6,8,10$ nodes
(in columns) and for each dimension, we build $1000, 2000, 5000,
10000, 20000, 50000$ trees (in rows). In order to be considered an
\emph{uniform random} generator, each $p$-value $v$ should be $.1
\leq v\ \leq .9$.
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{rrrrrr}
      \hline
      & 4 & 5 & 6 & 8 & 10 \\ 
      \hline
      1000 & 0.60 & 0.85 & 0.54 & 1.00 & 1.00 \\ 
      2000 & 0.99 & 0.98 & 0.13 & 1.00 & 1.00 \\ 
      5000 & 0.36 & 0.85 & 0.28 & 1.00 & 1.00 \\ 
      10000 & 0.69 & 0.73 & 0.11 & 0.65 & 1.00 \\ 
      20000 & 0.63 & 0.23 & 0.50 & 0.16 & 1.00 \\ 
      50000 & 0.09 & 0.74 & 0.63 & 0.90 & 1.00 \\ 
      \hline
    \end{tabular}
    \caption{$p$-values per trees and nodes}
    \label{table:p-values-per-nodes}
  \end{center}
\end{table}
Our simulation is quite good, there are two strange $p$-values for
dimension $2000$ and trees $4,5$ where we find $.99$ and $.98$. Maybe
the chance has played its role at the running time. Instead, the cell
containing $1$s means that the number of trees isn't sufficient to
perform a $\chi^2$ test (there are $16796$ different trees with $10$
nodes, with all simulated dimension isn't possible to test
randomness).

In table \autoref{table:empirical-trees-per-nodes} we report the
empirical number of trees calculated for trees with some nodes, each a
different number of times (as in the case above).
% latex table generated in R 2.15.1 by xtable 1.5-6 package
% Sun Dec 23 17:49:56 2012
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{rrrrrr}
      \hline
      & 4 & 5 & 6 & 8 & 10 \\ 
      \hline
      1000 & 11.08 & 31.69 & 128.86 & 1456.74 & 16770.17 \\ 
      2000 & 4.39 & 24.36 & 149.75 & 1454.88 & 17046.66 \\ 
      5000 & 14.18 & 31.62 & 140.13 & 1378.37 & 16478.72 \\ 
      10000 & 10.09 & 35.05 & 151.54 & 1414.26 & 17028.12 \\ 
      20000 & 10.75 & 47.34 & 130.37 & 1483.03 & 16813.47 \\ 
      50000 & 20.26 & 34.83 & 125.10 & 1359.54 & 16778.21 \\ 
      \hline
    \end{tabular}
    \caption{Empirical number of trees per nodes}
    \label{table:empirical-trees-per-nodes}
  \end{center}
\end{table}
We can see a correspondence of ``strange'' values in $p$-values table
and in the empirical number of trees table: where the $p$-values $v$
indicate a non random sequence we've a corresponding empirical value
very different respect the others in the same column.

It is possible to compare the values contained in table
\autoref{table:empirical-trees-per-nodes} with those (theoretically
correct) contained in \autoref{table:catalan-numbers}.
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{rrrrrr}
      \hline
      number of nodes & 4 & 5 & 6 & 8 & 10 \\ 
      number of trees & 14 & 42 & 132 & 1430 & 16796 \\ 
      \hline
    \end{tabular}
    \caption{Number of trees per nodes}
    \label{table:catalan-numbers}
  \end{center}
\end{table}

\section{Means of leaves and heights}

In this section we study the means of the number of leaves and of
heights of all different binary trees with $n$ nodes.

From theory we know that the mean of leaves of trees with $n$ nodes
is:
\begin{displaymath}
  \frac{n\,\left( n+1\right)  }{2\,\left( 2\,n-1\right) }
\end{displaymath}
Using Maxima to have the expacted means for some nodes dimensions:

\noindent
%%%%%%%%%%%%%%%
%%% INPUT:
\begin{minipage}[t]{8ex}{\color{red}\bf
\begin{verbatim}
(%i178) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fpprintprec:4$
leaves(n):=(n*(n+1))/(2*(2*n-1));
combineResult(n):=[n,leaves(n)]$
map(combineResult, makelist(n,n,1,10)),numer;
\end{verbatim}}
\end{minipage}
%%% OUTPUT:
\definecolor{labelcolor}{RGB}{100,0,0}
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o179) }
\mathrm{leaves}\left( n\right) :=\frac{n\,\left( n+1\right)
}{2\,\left( 2\,n-1\right) }
\end{math}\\
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o181) }
[[1,1],[2,1],[3,1.2],[4,1.429],[5,1.667],[6,1.909],[7,2.154],[8,2.4],
[9,2.647],[10,2.895]]
\end{math}\\
%%%%%%%%%%%%%%%

In \autoref{tab:means-of-leaves-height} we report the results
obtained with our implementation in R: the second column report the
number of trees generated with the correspondent number of nodes.
\begin{table}[!ht]
  \begin{center}
    \label{tab:means-of-leaves-height}
    \caption{Theoretical and empirical means for leaves and height}
    \rotatebox{90}{
      \begin{tabular}{rrrrrrr}
        \hline
        & nodes & dimensions & theo.mean.leaves & theo.mean.height & emp.mean.leaves & emp.mean.height \\ 
        \hline
         &   3 & 100 & 1.20 & 2.80 & 1.16 & 2.84 \\ 
         &   4 & 200 & 1.43 & 3.57 & 1.35 & 3.65 \\ 
         &   5 & 300 & 1.67 & 4.24 & 1.70 & 4.23 \\ 
         &   6 & 1000 & 1.91 & 4.88 & 1.91 & 4.87 \\ 
         &   7 & 10000 & 2.15 & 5.47 & 2.15 & 5.46 \\ 
         &   8 & 10000 & 2.40 & 6.03 & 2.41 & 6.02 \\ 
         &   9 & 50000 & 2.65 & 6.56 & 2.65 & 6.56 \\ 
         &  10 & 100000 & 2.89 & 7.07 & 2.90 & 7.07 \\ 
        \hline
      \end{tabular}
    }
  \end{center}
\end{table}

\section{Standardized means and asymptotic distributions}

In this section we study the asymptotic distribution of the means for
the leaves and heights of trees with $n$ nodes. We use the result of
the \emph{Central Limit Theorem} to check if the two means under study
behave like a normal distrubution in repeated sampling.

Each of the following plots are obtained repeating 1000 times the
generation of 500 trees each with 5 nodes. In each plot the red dotted
curve is the normal distribution (out ``target''), while the blue
continue curve is the inferred distribution.

In \autoref{fig:leaves-mean-distribution} we report the standardized
mean of leaves.
\begin{figure}[htb]
  \centering
  \includegraphics[height=13cm,
  width=13cm]{pictures/repeated-sampling-leaves-mean.pdf}
  \caption{Standardized leaves mean distribution}
  \label{fig:leaves-mean-distribution}
\end{figure}

In \autoref{fig:height-mean-distribution} we report the standardized
mean of leaves.
\begin{figure}[htb]
  \centering
  \includegraphics[height=13cm,
  width=13cm]{pictures/repeated-sampling-height-mean.pdf}
  \caption{Standardized height mean distribution}
  \label{fig:height-mean-distribution}
\end{figure}

\section{Drawing trees}

Just for fun, our implementation draw all different trees with $n$
nodes (this implementation is done in OCaml). In
\autoref{fig:binary-trees-with-four-nodes} we report the image with
all binary trees with $4$ nodes.
\begin{figure}[htb]
  \centering
  \rotatebox{90}{
    \includegraphics{pictures/binary-trees-with-four-nodes.png}
  }
  \caption{Binary trees with four nodes}
  \label{fig:binary-trees-with-four-nodes}
\end{figure}
